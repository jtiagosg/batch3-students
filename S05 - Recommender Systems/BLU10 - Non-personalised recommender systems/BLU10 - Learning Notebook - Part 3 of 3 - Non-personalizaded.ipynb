{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLU10 - Learning Notebook - Part 3 of 3 - Non-personalized Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Non-personalized RS\n",
    "\n",
    "The core functions of any RS is to identify useful items for the user.\n",
    "\n",
    "Going back to our framework, non-personalized RS typically include the base model (users, items, and ratings).\n",
    "\n",
    "We consider users, however, as providers of ratings, ignoring user preferences at recommendation time.\n",
    "\n",
    "![Recommender Sytems Framework](./media/recommender_systems_framework.png)\n",
    "\n",
    "*Fig.1 - RS framework with a community, the basic and extended models.*\n",
    "\n",
    "The rationale is that a generic user also likes something that is liked by many users.\n",
    "\n",
    "If we are unable to predict the utility of an item for a particular user, then we recommend an item with high utility for many users.\n",
    "\n",
    "This approach is particularly relevant in the absence of information about the user preferences.\n",
    "\n",
    "Non-personalized algorithms are useful to get a sense of building an RS and train our NumPy skills.\n",
    "\n",
    "# 2 Loading the data\n",
    "\n",
    "First, we read the data into Python and NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    \n",
    "    path = os.path.join('data', 'ml-latest-small', 'ratings.csv')\n",
    "    data = np.genfromtxt(path, delimiter=',', skip_header=1, usecols=[0, 1, 2])\n",
    "    return data\n",
    "\n",
    "\n",
    "data = read_data()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the presentation when printing is not anything refined, like in Pandas, what we have here is:\n",
    "* User identification in the first column\n",
    "* Item identification in the second column\n",
    "* Rating in the third column.\n",
    "\n",
    "This array is, in short, our set of recorded ratings $R'$, presented in long-form.\n",
    "\n",
    "For more information, explore the `../data/ml-latest-small/` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Building the ratings matrix\n",
    "\n",
    "The second step then is to transform this representation into a ratings matrix, with:\n",
    "* Users as rows\n",
    "* Items as columns\n",
    "* Ratings as values.\n",
    "\n",
    "We use the unique values for users and items, storing the indices that can be used to reconstruct the original array.\n",
    "\n",
    "Then, we create a matrix, all filled with zeros, the size we want:\n",
    "* The number of unique users is the number of rows\n",
    "* The number of unique items is the number of columns.\n",
    "\n",
    "Finally, we fill in the rating values, using the stored indexes, in a vectorized way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ratings(data):\n",
    "    \n",
    "    users, user_pos = np.unique(data[:, 0], return_inverse=True)\n",
    "    items, item_pos = np.unique(data[:, 1], return_inverse=True)\n",
    "    \n",
    "    R = np.zeros((len(users), len(items)))\n",
    "    R[user_pos, item_pos] = data[:, 2]\n",
    "    \n",
    "    return R\n",
    "\n",
    "\n",
    "R = make_ratings(data)\n",
    "R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take your time to read through and experiment with the code as you go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Sparsity score\n",
    "\n",
    "Now, we compute the sparsity score of the ratings matrix.\n",
    "\n",
    "We will use the array method `nonzero` to return a mask of the element that are non-zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R.nonzero()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we've seen, we compute the sparsity score, as: \n",
    "\n",
    "$$Sparcity = \\frac{|R'|}{|R|}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R[R.nonzero()].size / R.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Holy moly, at least now we know what we are up against!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Aggregated opinion\n",
    "\n",
    "Again, the most important idea about non-personalization is that we predict the utility for the entire community.\n",
    "\n",
    "Perhaps the oldest RS is aggregated opinion, i.e., most popular/hated (Think Billboard or [IMDb Bottom 100](https://www.imdb.com/chart/bottom)).\n",
    "\n",
    "## 5.1 Most-rated\n",
    "\n",
    "We can think of the most popular as the most rated.\n",
    "\n",
    "We start by checking which elements are greater than zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_rating(R):\n",
    "    return np.greater(R, 0)\n",
    "\n",
    "\n",
    "is_rating(R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recalling that each row corresponds to user and each column to an item, we can sum the results in each column to know how many ratings exist for that item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_ratings(R):\n",
    "    R_ = is_rating(R)\n",
    "    return R_.sum(axis=0)\n",
    "\n",
    "\n",
    "count_ratings(R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can have a function that retrieves the top-$N$ most-rated items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_rated(R, n):\n",
    "    R_ = count_ratings(R)\n",
    "    return np.negative(R_).argsort()[:n]\n",
    "\n",
    "\n",
    "most_rated(R, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 % > X\n",
    "\n",
    "We can extend the function above to mimic another popular algorithm, \"% of people that like this item\".\n",
    "\n",
    "Let's say a positive rating is anything above the value of 3 (e.g., 3 stars)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_positive_ratings(R, threshold):\n",
    "    R_ = is_above_threshold(R, threshold)\n",
    "    return R_.sum(axis=0)\n",
    "\n",
    "\n",
    "def is_above_threshold(R, threshold):\n",
    "    return np.greater(R, threshold)\n",
    "\n",
    "\n",
    "count_positive_ratings(R, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we just need to count the number of positive ratings and sort the resulting array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_positive_ratings(R, threshold, n):\n",
    "    R_ = count_positive_ratings(R, threshold)\n",
    "    return np.negative(R_).argsort()[:n]\n",
    "\n",
    "\n",
    "most_positive_ratings(R, 3, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Summary statistics\n",
    "\n",
    "Probaly the most popular non-personalized algorithm is the average rating.\n",
    "\n",
    "Popularized at first by Amazon and Ebay and then IMDB, Netflix, among others, this is a basic yet widely used algorithm.\n",
    "\n",
    "The first step is to remove the zeros, so that they don't affect out average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_zeros(R):\n",
    "    R_ = R.copy()\n",
    "    R_[R_ == 0] = np.NaN\n",
    "    \n",
    "    return R_\n",
    "\n",
    "\n",
    "remove_zeros(R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A now, we can safely compute the average rating per item and sort the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_ratings(R):\n",
    "    R_ = remove_zeros(R)\n",
    "    return np.nanmean(R_, axis=0)\n",
    "\n",
    "\n",
    "mean_ratings(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_mean_rating(R, n):\n",
    "    R_ = mean_ratings(R)\n",
    "    return np.negative(R_).argsort()[:n]\n",
    "\n",
    "\n",
    "best_mean_rating(R, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are alternatives, such as computing the \"mean rating for users that liked this item\", that we don't explore.\n",
    "\n",
    "It's increasingly popular also to show an histogram alongside mean ratings, to give a sense of the distribution of ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Association rules\n",
    "\n",
    "Perhaps one of the most interesting (and also very popular) non-personalized algorithms is \"people that buy X, also buy Y\".\n",
    "\n",
    "These are called association rules. Here, and for the sake of conciseness, we use `mlxtend` to implement some of them. \n",
    "\n",
    "(Yes, we are cheating. We should be implementing it with NumPy!)\n",
    "\n",
    "## 7.1 Apriori\n",
    "\n",
    "Apriori is used to identify common item pairs, i.e., stuff that usually goes together:\n",
    "* We identify individual items that satisfy a minimum occurrence threshold\n",
    "* Then, we extend the item sets, adding one item at a time \n",
    "* Every time we check if the resulting item set satisfies the specified threshold\n",
    "* The algorithm stops when there are no more items to add that meet the threshold. \n",
    "\n",
    "The `mlxtend` expects a one-hot input, i.e., 0/1 or True/False.\n",
    "\n",
    "(Unfortunately, `mlxtend` only supports dataframes at this point. We still cheating.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_apriori_itemsets(R, min_support=0.3):\n",
    "    R_ = pd.DataFrame(R > 0)\n",
    "    return apriori(R_, min_support)\n",
    "\n",
    "\n",
    "get_apriori_itemsets(R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Support\n",
    "\n",
    "Support is the percentage of users that contains the item set, so:\n",
    "\n",
    "$$Support\\{i, j\\} = \\frac{|U_{i, j}|}{|U|} = \\frac{|U_{i, j}|}{m}$$\n",
    "\n",
    "## 7.3 Confidence\n",
    "\n",
    "Given two sets, the confidence is the how frequently the item $j$ is purchased, given that item $i$ was purchased, as:\n",
    "\n",
    "$$Confidence\\{i \\to j \\} = \\frac{Support\\{i, j\\}}{Support\\{i\\}}$$\n",
    "\n",
    "Or, in a more familiar way, confidence is the conditional probability of $j$ given $i$:\n",
    "\n",
    "$$P(j|i) = \\frac{P(i \\cap j)}{P(i)}$$\n",
    "\n",
    "However, do $i$, and $j$ occur for the same users for a reason, or is it random? What if $j$ is a trendy item?\n",
    "\n",
    "## 7.4 Lift\n",
    "\n",
    "Meet the bananas trap: just because people buy bananas most times, it doesn't mean bananas go well with soap.\n",
    "\n",
    "Fortunately, there is a better way. \n",
    "\n",
    "The lift algorithm, which takes into consideration the popularity of the items.\n",
    "\n",
    "$$Lift\\{i, j\\} = \\frac{Support\\{i, j\\}}{Support\\{i\\} * Support\\{j\\}}$$\n",
    "\n",
    "The denominator is the likelihood that $i$ and $j$ appear together by chance, so lift questions whether $i$ makes $j$ more probable or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rules(R, min_support=.3, min_threshold=1.2):\n",
    "    itemsets = get_apriori_itemsets(R, min_support=0.3)\n",
    "    return association_rules(itemsets, metric=\"lift\", min_threshold=min_threshold)\n",
    "\n",
    "\n",
    "get_rules(R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have the foundations to tackle more complext recommendation approaches.\n",
    "\n",
    "Time to practice!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
